{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7962b3dd",
   "metadata": {},
   "source": [
    "# Introduction to automatic differentiation\n",
    "\n",
    "Automatic differentiation is a technique to automatically compute the derivative of a function, where the function is written as a composition of elementary sub-functions that we know how to differentiate in closed form. It is a cornerstone in modern machine learning, and can be useful in any science where coding is involved.\n",
    "\n",
    "In practice, assume that we also have some Python function\n",
    "```python\n",
    "def func(x):\n",
    "    ...\n",
    "    return y\n",
    "```\n",
    "where `x` is a Numpy array of size $p$ and `y` is a numpy array of size $q$, and `...` is a succession of operations that we can differentiate in closed form. Then, `func` implements a differentiable function\n",
    "$$f:\\mathbb{R}^p\\to \\mathbb{R}^q.$$\n",
    "\n",
    "Two quantities of interest for $f$ are **Jacobian-Vector-Products** (JVP's) and **Vector-Jacobian-Products** (VJP's), which can be computed automatically from the code of `func`.\n",
    "\n",
    "Letting $J(x)\\in\\mathbb{R}^{q\\times p}$ the Jacobian matrix of $f$ at $x$, the Jacobian-vector product of $f$ at $x$ with a vector $v\\in \\mathbb{R}^p$ is simply $J(x)v\\in\\mathbb{R}^q$.\n",
    "\n",
    "The Vector-Jacobian-Product with a vector $v'\\in\\mathbb{R}^q$ is $J(x)^Tv'\\in\\mathbb{R}^p$.\n",
    "\n",
    "**Key remark**: automatic differentiation allows to compute these JVP's-VJP's efficiently, at roughly the same cost as evaluating the function `func` itself! Hence, it does not work by first computing the big matrix $J(x)$ and then multiplying it by a vector; it is much more efficient than this.\n",
    "\n",
    "\n",
    "**Question 1:** In machine learning, we are often interested in computing the gradient of cost functions, i.e. functions that go from $\\mathbb{R}^p$ to $\\mathbb{R}$. Can you compute the gradient of $f$ using a JVP? Using a VJP? Which one is the most convenient?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6683e8b9",
   "metadata": {},
   "source": [
    "## Forward mode automatic differentiation\n",
    "\n",
    "Forward mode automatic differentiation is a method to compute JVP's. Even though in machine learning we usually want to compute VJP's, we begin with forward mode automatic differentiation because it is conceptually more natural.\n",
    "\n",
    "The idea of forward mode automatic differentiation is simply to follow the same steps as in the code of the function, while applying at the same time the rule of differentiation.\n",
    "\n",
    "For instance, we consider the following toy example of ``func`` (it is just a random function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1611bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd807745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.84147098, 3.2431975 , 1.84147098]),\n",
       " array([-2.35507598,  3.07215118]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 3\n",
    "x = np.array([1., 2., -1.])\n",
    "A = np.array([[0., 2., -1.], [3., 1.5, 2.]])\n",
    "\n",
    "def func(x):\n",
    "    # x is a 3-dimensional array\n",
    "    y = x ** 2\n",
    "    z = np.sin(y)\n",
    "    t = A @ z\n",
    "    return y + z, t\n",
    "\n",
    "func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81670e6",
   "metadata": {},
   "source": [
    "This function takes as input something of size $3$ and outputs two vectors of size $3$ and $2$ respectively. \n",
    "\n",
    "We can compute \"automatically\" its JVP with a vector $v \\in \\mathbb{R}^3$ by following the rules of differentiation. \n",
    "\n",
    "To do so by hand, we copy and paste the code, and add the missing differentiation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d52bb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jvp(x, v):\n",
    "    # x is a 3-dimensional array\n",
    "    \n",
    "    y = x ** 2\n",
    "    vy = 2 * x * v  # this is dy / dx * v\n",
    "    \n",
    "    z = np.sin(y)\n",
    "    vz = np.cos(y) * vy  # this is dz / dx * v\n",
    "    \n",
    "    t = A @ z\n",
    "    vt = A @ vz # this is dt / dx * v\n",
    "    return vy + vz, vt  # this is df / dx * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "852fb2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.08060461,   4.15627655, -12.32241845]),\n",
       " array([-11.36502845, -17.16860823]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([1., 3., 4.])\n",
    "\n",
    "jvp(x, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f2577",
   "metadata": {},
   "source": [
    "Importantly, forward-mode automatic differentiation works by keeping track at each step of the variation of the current variable with respect to the **input**.\n",
    "\n",
    "We can check that these computations are correct, simply by comparing with an approximation of the JVP, given by:\n",
    "\n",
    "$$\n",
    "J(x)v \\simeq \\frac{f(x+\\varepsilon v) - f(x)}{\\varepsilon}\n",
    "$$\n",
    "for $\\varepsilon$ a small scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c718601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.0806046 ,   4.15628231, -12.32241868]),\n",
       " array([-11.3650169 , -17.16860493]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def approximate_jvp(x, v, eps=1e-7):\n",
    "    return tuple((a - b) / eps for a, b in zip(func(x + eps * v), func(x)))\n",
    "\n",
    "\n",
    "approximate_jvp(x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59df4009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.08060461,   4.15627655, -12.32241845]),\n",
       " array([-11.36502845, -17.16860823]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jvp(x, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9edd4",
   "metadata": {},
   "source": [
    "We see that we have done a good job: both computations lead to almost the same result. \n",
    "\n",
    "A natural question is now: why implement the JVP with the approximation technique?\n",
    "The main reason is that this technique is not exact; it has some error driven by $\\varepsilon$. On the other hand, the automatic differentiation is exact up to machine precision.\n",
    "\n",
    "You are now tasked to use the same method to implement the JVP in a simple two-layer neural network, defined by:\n",
    "\n",
    "$$ f(x) = A\\sigma(Bx),$$\n",
    "\n",
    "where $B \\in\\mathbb{R}^{n\\times p}$, $A\\in\\mathbb{R}^{q\\times n}$ and $\\sigma:\\mathbb{R}^n\\to\\mathbb{R}^n$ is a non-linear function, defined by $\\sigma(y_1, \\dots, y_n) = (y_1^2, \\dots, y_n^2)$.\n",
    "\n",
    "We give below the code of the function.\n",
    "\n",
    "Beware that it takes as inputs `x, B, A`, hence the vector `v` in the JVP will contain 3 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a879c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layers(x, B, A):\n",
    "    y = B @ x\n",
    "    z = y ** 2\n",
    "    return A @ z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f1044cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44313108, 0.19279774, 0.1266412 ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 2\n",
    "n = 4\n",
    "q = 3\n",
    "\n",
    "x = np.random.randn(p)\n",
    "B = np.random.randn(n, p)\n",
    "A = np.random.randn(q, n)\n",
    "\n",
    "two_layers(x, B, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1b283",
   "metadata": {},
   "source": [
    "Now, fill in the blanks in the next function to compute the JVP with automatic differentiation; and check that this is correct with approximate differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "406d58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layers_jvp(x, B, A, v):\n",
    "    vx, vB, vA = v  # v is a tuple\n",
    "    \n",
    "    y = B @ x\n",
    "    vy = # TODO\n",
    "    \n",
    "    z = y ** 2\n",
    "    vz = # TODO\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f81b6411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.39764278, -6.37349448, -0.50283813])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vx = np.random.randn(p)\n",
    "vB = np.random.randn(n, p)\n",
    "vA = np.random.randn(q, n)\n",
    "\n",
    "v = (vx, vB, vA)\n",
    "two_layers_jvp(x, B, A, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64944046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_jvp(x, B, A, v, eps=1e-7):\n",
    "    vx, vB, vA = v\n",
    "    return (two_layers(x + eps * vx, B + eps * vB, A + eps * vA) - two_layers(x, B, A))/ eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d00a42f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.39764268, -6.37349406, -0.50283775])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approximate_jvp(x, B, A, v)  # this should be close to the output of your two_layers_jvp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d4974",
   "metadata": {},
   "source": [
    "Forward-mode automatic differentiation is rarely used in machine learning, because for a function $f:\\mathbb{R}^p\\to \\mathbb{R}$ it only computes $\\nabla f(x)^T v$ for a vector $v$.\n",
    "\n",
    "What is used in practice for real-valued functions is rather backward automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089134c",
   "metadata": {},
   "source": [
    "## Backward automatic differentiation (or backpropagation for real-valued functions)\n",
    "\n",
    "The main idea is the following:\n",
    "- Forward mode automatic differentiation tracks the derivative of each variable with respect to the **input**.\n",
    "- Backward mode automatic differentiation tracks the derivative of the **output** with respect to each variable.\n",
    "\n",
    "Hence, backward-mode computes things in a backwards order: it goes backward through the computational graph of the function.\n",
    "\n",
    "The key point here, and if this is the first time you see this it might seem a bit weird, is that the classical rules of differentiation also apply backwards.\n",
    "\n",
    "Let us take the same example as before, and try to compute a Vector-Jacobian Product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "681f778a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.84147098, 3.2431975 , 1.84147098]),\n",
       " array([-2.35507598,  3.07215118]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 3\n",
    "x = np.array([1., 2., -1.])\n",
    "A = np.array([[0., 2., -1.], [3., 1.5, 2.]])\n",
    "\n",
    "def func(x):\n",
    "    # x is a 3-dimensional array\n",
    "    y = x ** 2\n",
    "    z = np.sin(y)\n",
    "    t = A @ z\n",
    "    return y + z, t\n",
    "\n",
    "func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25bc3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vjp(x, v):\n",
    "    # x is a 3-dimensional array\n",
    "    # the function outputs two arrays, hence now v is two arrays\n",
    "    vu, vt = v\n",
    "    \n",
    "    # we first run the function code, without differentiating anything. We keep each intermediate variable in memory ! \n",
    "    y = x ** 2\n",
    "    z = np.sin(y)\n",
    "    t = A @ z\n",
    "    output_u = y + z\n",
    "    output_t = t\n",
    "    # Now, we \" backprop\" through the computational graph, starting from the end.\n",
    "    # Here, I comment each line of code and write the corresponding differentiation below.\n",
    "    \n",
    "    # output_t = t\n",
    "    vt = vt  # This is just the derivative of output_t in the direction vt\n",
    "    \n",
    "    # output_u = y + z\n",
    "    vy = vu.copy()  # this is d output / dy in the direction (vu, vt). By \"in the direction\", we just mean the VJP.\n",
    "    vz = vu.copy()  # this is d output/ dz in the direction (vu, vt).\n",
    "    \n",
    "    # t = A @ z\n",
    "    vz += A.T @ vt  # we have already seen z in the graph, so we accumulate the derivatives.\n",
    "    \n",
    "    # z = np.sin(y)\n",
    "    vy += np.cos(y) * vz  # remember that vy is the derivative of the output w.r.t. y at the current point of the graph.\n",
    "    # the previous equation can be rewritten as doutput / dy = doutput/dz * dz/dy, and we have doutput/dz = vz, and from the line z = sin(y), we also have dz/dy = cos(y).\n",
    "    \n",
    "    # y = x ** 2\n",
    "    vx = 2 * x * vy  # same as just above. This might be counter intuitive, so take your time to reflect on this.\n",
    "    return vx  # we have finished traversing the computational graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "884608d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.10806046, -0.77655852, -8.16120922])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vu = np.array([1., 0.1, 3.])\n",
    "vt = np.array([0.4, -0.3])\n",
    "v = vu, vt\n",
    "vjp(x, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cdbda",
   "metadata": {},
   "source": [
    "We can check that this is correct by computing the scalar product with a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11ae3fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.194159242052528"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = np.array([2., 3., 0.5])\n",
    "\n",
    "np.dot(vjp(x, v), v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aeb1bf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1941592420525278"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.dot(a, b) for a, b in zip(jvp(x, v2), v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12284e",
   "metadata": {},
   "source": [
    "Now, it is your turn to implement this method to compute a VJP through the two layers neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e7a2a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layers(x, B, A):\n",
    "    y = B @ x\n",
    "    z = y ** 2\n",
    "    return A @ z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d241e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.39956728,  0.35436586, -0.69567035])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 2\n",
    "n = 4\n",
    "q = 3\n",
    "\n",
    "x = np.random.randn(p)\n",
    "B = np.random.randn(n, p)\n",
    "A = np.random.randn(q, n)\n",
    "\n",
    "two_layers(x, B, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c937d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.random.randn(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c10c8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layers_vjp(x, B, A, v):\n",
    "    # This should output vx, vB, vA.\n",
    "    y = B @ x\n",
    "    z = y ** 2\n",
    "    output = A @ z\n",
    "    # Now, backprop through the computations:\n",
    "    # output = A @ z\n",
    "    vz = # TODO\n",
    "    vA = # TODO\n",
    "    \n",
    "    # z = y ** 2\n",
    "    vy = # TODO\n",
    "    # y = B @ x\n",
    "    vB = # TODO\n",
    "    vx = # TODO\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02efd2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layers_vjp(x, B, A, v)\n",
    "vx = np.random.randn(p)\n",
    "vB = np.random.randn(n, p)\n",
    "vA = np.random.randn(q, n)\n",
    "\n",
    "v2 = (vx, vB, vA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a85f273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18.80730340627934"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.sum(a * b) for a, b in zip(two_layers_vjp(x, B, A, v), v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "68fcf4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18.807303406279335"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(two_layers_jvp(x, B, A, v2) * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f2bb2",
   "metadata": {},
   "source": [
    "These numbers should be the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057f86e",
   "metadata": {},
   "source": [
    "Now, imagine that we add a loss function on top of it, for instance defined as $f(x, A, B) = \\sum_{i=1}^q [\\texttt{twolayers}(x, A, B)]_i^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "764c060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, B, A):\n",
    "    z = two_layers(x, B, A)\n",
    "    return np.sum(z ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac53e7",
   "metadata": {},
   "source": [
    "Using the previous VJP function `two_layers_jvp`, compute the gradient of the function with respect to (x, B, A):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a32efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x, B, A):\n",
    "    # forward computations\n",
    "    z = two_layers(x, B, A)\n",
    "    output = np.sum(z ** 2)\n",
    "    # Backward computations:\n",
    "    doutputdz = # TODO\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "095b2732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 15.82332274, -48.38062266]),\n",
       " array([[-29.52150855,   4.26020931],\n",
       "        [-22.3080378 ,   3.21924302],\n",
       "        [-20.95312919,   3.02371798],\n",
       "        [ 42.21605038,  -6.09214162]]),\n",
       " array([[ 4.04937186, 42.46766857,  2.60546593,  5.59341797],\n",
       "        [ 0.42210053,  4.42676691,  0.27158991,  0.58304961],\n",
       "        [-0.82864309, -8.69037012, -0.53316944, -1.14460892]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(x, B, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5b8cf",
   "metadata": {},
   "source": [
    "We will check below that this is correct.\n",
    "\n",
    "**Question:** What is the cost of computing this gradient, compared to the cost of computing the loss function itself?\n",
    "\n",
    "\n",
    "We see here the main advantage of reverse-mode automatic differentiation: it is an exact method to compute the gradient of a function, at roughly the same cost as computing the function itself !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7953af",
   "metadata": {},
   "source": [
    "## Drawback\n",
    "\n",
    "The main drawback of backpropagation is that we need to keep in memory every intermediate state in the code; therefore it is costly in terms of memory. Forward mode automatic differentiation does not have such a problem: we can throw away variables that are not going to be used anymore at evaluation time,  but it is impractical to compute gradients. \n",
    "\n",
    "Hence, in deep learning, it is much more costly in terms of memory to train a neural network, where we have to use backpropagation, compared to just computing the output of the network without backprop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f8510",
   "metadata": {},
   "source": [
    "## Autodiff in practice\n",
    "\n",
    "So far, we have coded by hand automatic differentiation; this is of course highly impractical. Thnakfully, any modern deep learning framework like Pytorch or Jax has an automatic differentiation engine, which means that they can compute JVP / VJP/ gradients using only the code of the function that you provide. What happens under the hood is exactly what we have coded above; but it is hidden to the user. \n",
    "\n",
    "We will demonstrate it using Jax.\n",
    "\n",
    "Jax has a syntax really close to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "901ff773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3a847964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([1.841471 , 3.2431974, 1.841471 ], dtype=float32),\n",
       " Array([-2.3550758,  3.0721512], dtype=float32))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 3\n",
    "x = jnp.array([1., 2., -1.])\n",
    "A = jnp.array([[0., 2., -1.], [3., 1.5, 2.]])\n",
    "\n",
    "def func(x):\n",
    "    # x is a 3-dimensional array\n",
    "    y = x ** 2\n",
    "    z = jnp.sin(y)  # need to put jnp instead of np\n",
    "    t = A @ z\n",
    "    return y + z, t\n",
    "\n",
    "func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac51af9c",
   "metadata": {},
   "source": [
    "To compute the JVP of the function, simply use `jax.jvp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "669e93f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([  3.0806046,   4.1562767, -12.322418 ], dtype=float32),\n",
       " Array([-11.365028, -17.168608], dtype=float32))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([1., 3., 4.])\n",
    "\n",
    "jax.jvp(func, (x,), (v,))[1]  # the first component is just func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e0e97",
   "metadata": {},
   "source": [
    "This should match what we have done above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3205fb59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([  3.0806046,   4.1562767, -12.322418 ], dtype=float32),\n",
       " Array([-11.365028, -17.168608], dtype=float32))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jvp(x, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4b96f",
   "metadata": {},
   "source": [
    "Let us focus on computing gradients in Jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2a35f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2\n",
    "n = 4\n",
    "q = 3\n",
    "\n",
    "x = np.random.randn(p)\n",
    "B = np.random.randn(n, p)\n",
    "A = np.random.randn(q, n)\n",
    "\n",
    "two_layers(x, B, A)\n",
    "\n",
    "def loss(x, B, A):\n",
    "    z = two_layers(x, B, A)\n",
    "    return jnp.sum(z ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666b3fc",
   "metadata": {},
   "source": [
    "The `jax.grad` function does this exactly. By default, it differentiates only with respect to the first parameter, but we can tell it to differentiate with respect to specific arguments using the `argnum` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "988d8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = jax.grad(loss, argnums=[0, 1, 2])  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1870041",
   "metadata": {},
   "source": [
    "`grad` is now a function, that computes the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7860f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-126.91913,  -75.35017], dtype=float32),\n",
       " Array([[ 36.090748 ,   3.246026 ],\n",
       "        [ 69.692474 ,   6.2681875],\n",
       "        [ 22.129803 ,   1.9903693],\n",
       "        [-49.829563 ,  -4.481704 ]], dtype=float32),\n",
       " Array([[102.664856  ,  86.8243    ,   4.415657  ,  20.893446  ],\n",
       "        [-10.290272  ,  -8.702547  ,  -0.44258875,  -2.0941854 ],\n",
       "        [ 83.49165   ,  70.609406  ,   3.5910094 ,  16.991484  ]],      dtype=float32))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(x, B, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc4e97",
   "metadata": {},
   "source": [
    "Therefore, automatic differentiation is already implemented in Jax!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
